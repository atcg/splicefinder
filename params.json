{"name":"SpliceFinder","tagline":"","body":"\r\nSplicefinder\r\n======\r\n\r\nThis is the walkthrough and code that accompanies \"Conservative prediction of intron splice sites for the design of exon-capture arrays.\"\r\n\r\n\r\n\r\n###Step 1: Download mouse data from Ensembl\r\n\r\n_________________\r\n\r\nConnect to the server, navigate to the proper folder, and download data in bash:\r\n\r\n```bash\r\nssh maven\r\ncd /mnt/Data1/spliceFinder\r\nmkdir ensembl\r\ncd ensembl\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/mus_musculus/cdna/Mus_musculus.GRCm38.cdna.all.fa.gz\r\n\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/mus_musculus/cdna/README\r\n\r\ngunzip Mus_musculus.GRCm38.cdna.all.fa.gz\r\n```\r\n\r\nAfter downloading the data, `get_fasta_lengths.py --input Mus_musculus.GRCm38.cdna.all.fa` gives the following output:\r\n\r\n```text\r\nReads:\t\t90,891\r\nBp:\t\t168,812,502\r\nAvg. len:\t1,857.30712612\r\nSTDERR len:\t6.5011432467\r\nMin. len:\t9\r\nMax. len:\t106,824\r\nMedian len:\t1,078.0\r\nContigs > 1kb:\t47,261\r\n```\r\n\r\n\r\nThese represent the latest set of mouse cDNAs from ENSEMBL. We are first going to randomly subset these into fewer than 90,891, because we are going to run many analyses on these sets, mapping to multiple genomes (which can be slow). Let's see what they look like:\r\n\r\n```text\r\nhead --lines 6 Mus_musculus.GRCm38.cdna.all.fa\r\n>ENSMUST00000196221 havana_ig_gene:known chromosome:GRCm38:14:54113468:54113476:1 gene:ENSMUSG00000096749 gene_biotype:TR_D_gene transcript_biotype:TR_D_gene\r\nATGGCATAT\r\n>ENSMUST00000179664 ensembl_ig_gene:known chromosome:GRCm38:14:54113468:54113478:1 gene:ENSMUSG00000096749 gene_biotype:TR_D_gene transcript_biotype:TR_D_gene\r\nATGGCATATCA\r\n>ENSMUST00000177564 ensembl_havana_ig_gene:known chromosome:GRCm38:14:54122226:54122241:1 gene:ENSMUSG00000096176 gene_biotype:TR_D_gene transcript_biotype:TR_D_gene\r\nATCGGAGGGATACGAG\r\n```\r\n\r\nLooking at this output tells us a few things. First of all, some of the transcripts are extremely short. Since the purpose of this method is to infer contiguous genomic sequence chunks long enough to use target enrichment array design, we know we'll want to filter out all cDNAs that are too short to actually yield a target. We'll arbitrarily set this limit at 200bp for now. The following script will output all sequences longer than 199bp:\r\n\r\n```perl\r\n#!/usr/bin/perl\r\n\r\n#filterUnder200.pl\r\n\r\nuse strict;\r\nuse warnings;\r\nuse Bio::SeqIO;\r\n\r\nmy $seqIn = Bio::SeqIO->new(-file => \"Mus_musculus.GRCm38.cdna.all.fa\",\r\n                            -format => 'fasta');\r\nmy $seqOut = Bio::SeqIO->new(-file => \">mmGRCm38.cdna.200bpPlus.fa\",\r\n                             -format => \"fasta\");\r\n\r\nwhile (my $seq = $seqIn->next_seq()) {\r\n    if ($seq->length() > 199) {\r\n        $seqOut->write_seq($seq);\r\n    }\r\n}\r\n```\r\n\r\nAfter running, we have:\r\n\r\n```text\r\nget_fasta_lengths.py --input mmGRCm38.cdna.200bpPlus.fa \r\nReads:          90,060\r\nBp:             168,702,266\r\nAvg. len:       1,873.22080835\r\nSTDERR len:     6.53762906167\r\nMin. len:       200\r\nMax. len:       106,824\r\nMedian len:     1,098.0\r\nContigs > 1kb:  47,261\r\n```\r\n\r\nThat didn't very many, so we'll have to reduce further. You'll notice in the fasta headers that there is a gene name in the fourth column. Let's find out how many unique gene names there are total:\r\n\r\n```text\r\ngrep \"gene:\" mmGRCm38.cdna.200bpPlus.fa | awk '{print $4}' | uniq -c | wc -l\r\n30722\r\n```\r\n\r\nRandomly choosing one transcript from each gene name seems like one good way to reduce the dataset without losing genomic coverage. But choosing the longest cDNA from each gene name might give us better success at mapping to other genomes. Let's do that with the following script:\r\n\r\n```perl\r\n#!/usr/bin/perl\r\n\r\n# longestFromGenes.pl\r\n\r\nuse strict;\r\nuse warnings;\r\nuse Bio::SeqIO;\r\n\r\n\r\nmy %seqHash;\r\nmy $seqIn1 = Bio::SeqIO->new(-file => \"mmGRCm38.cdna.200bpPlus.fa\",\r\n                             -format => \"fasta\");\r\nmy $seqOut = Bio::SeqIO->new(-file => \">mmGRCm38.cdna.longestFromGenes.fa\",\r\n                             -format => \"fasta\");\r\n\r\nwhile (my $seq = $seqIn1->next_seq()) {\r\n    if ($seq->desc() =~ /.*gene\\:(\\S+)\\s/) {\r\n        # Each transcript is associated with a gene name, and has a\r\n        # hash value equal to its length\r\n        my $geneName = $1;\r\n        $seqHash{$geneName}{$seq->display_id()} = $seq;\r\n    }\r\n}\r\n\r\nforeach my $gene (sort keys %seqHash) {\r\n    my $longestID;\r\n    my $longestLength = 0;\r\n    foreach my $transcript (sort keys %{$seqHash{$gene}}) {\r\n        if ($seqHash{$gene}{$transcript}->length() > $longestLength) {\r\n            $longestID = $transcript;\r\n            $longestLength = $seqHash{$gene}{$transcript}->length();\r\n        }\r\n    }\r\n    $seqOut->write_seq($seqHash{$gene}{$longestID})\r\n}\r\n```    \r\n\r\n\r\nThis should get the list down to 30,722, and the average length should go up (it was 1,873 bp before).\r\n\r\n```text\r\nget_fasta_lengths.py --input mmGRCm38.cdna.longestFromGenes.fa \r\nReads:          30,722\r\nBp:             73,817,860\r\nAvg. len:       2,402.76869995\r\nSTDERR len:     13.0889977284\r\nMin. len:       200\r\nMax. len:       106,824\r\nMedian len:     1,734.0\r\nContigs > 1kb:  20,279\r\n```\r\n\r\nLooking good! I think 30,722 transcripts is still a little high though, so I will subset the dataset down to 10,000 transcripts randomly.\r\n\r\n```perl\r\n#!/usr/bin/perl\r\nuse strict;\r\nuse warnings;\r\nuse List::Util qw/shuffle/;\r\n\r\n#random10k.pl\r\n\r\nmy $seqIn = Bio::SeqIO->new(-file => \"mmGRCm38.cdna.longestFromGenes.fa\",\r\n                            -format => \"fasta\");\r\nmy $seqOut = Bio::SeqIO->new(-file => \">mmGRCm38.cdna.rand10kLongest.fa\",\r\n                            -format => \"fasta\");\r\n\r\nmy @seqArray;\r\nwhile (my $seq = $seqIn->next_seq()) {\r\n    push(@seqArray, $seq);\r\n}\r\n\r\nmy @shuffledSeqs = shuffle(@seqArray);\r\n\r\nmy $counter = 0;\r\nwhile ($counter < 10000) {\r\n    $seqOut->write_seq($shuffledSeqs[$counter]);\r\n    $counter++;\r\n}\r\n```\r\n\r\nThat should give us 10,000 transcripts from 10,000 distinct genes scattered throughout the mouse genome.\r\n\r\n```text\r\nget_fasta_lengths.py --input mmGRCm38.cdna.rand10kLongest.fa\r\nReads:\t\t10,000\r\nBp:\t\t23,940,240\r\nAvg. len:\t2,394.024\r\nSTDERR len:\t21.8914472689\r\nMin. len:\t200\r\nMax. len:\t22,489\r\nMedian len:\t1,732.0\r\nContigs > 1kb:\t6,651\r\n```\r\n\r\nExcellent. Now we're ready to set up some mapping. In this experiment, we're going to want to map to approximately 10 genomes of varying sequence divergence from the mouse genome. For now, let's try the following genomes:\r\n  1. Rat (_Rattus norvegicus_)\r\n  2. Human (_Homo sapiens_)\r\n  3. Pig (_Sus scrofa_)\r\n  4. Elephant (_Loxodonta_africana_)\r\n  5. Playtpus (_Ornithorynchus anatinus_)\r\n  6. Chicken (_Gallus gallus_)\r\n  7. _Xenopus tropicalis_\r\n  8. _Anolis carolinensis_\r\n  9. Coelacanth (_Latimeria chalumnae_)\r\n  10. Puffer fish (_Takifugu rubripes_)\r\n  11. Sea lamprey (*Petromyzon marinus*)\r\n  12. Sea urchin (*Strongylocentrotus purpuratus*)\r\n\r\n\r\nLet's download those genomes, getting the unmasked primary assembly if possible, otherwise taking the unmasked top-level assembly:\r\n\r\n```bash\r\nmkdir genomes\r\ncd genomes\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/rattus_norvegicus/dna/Rattus_norvegicus.Rnor_6.0.dna.toplevel.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/sus_scrofa/dna/Sus_scrofa.Sscrofa10.2.dna.toplevel.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/loxodonta_africana/dna/Loxodonta_africana.loxAfr3.dna.toplevel.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/ornithorhynchus_anatinus/dna/Ornithorhynchus_anatinus.OANA5.dna.toplevel.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/gallus_gallus/dna/Gallus_gallus.Galgal4.dna.toplevel.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/xenopus_tropicalis/dna/Xenopus_tropicalis.JGI_4.2.dna.toplevel.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/anolis_carolinensis/dna/Anolis_carolinensis.AnoCar2.0.dna.toplevel.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/latimeria_chalumnae/dna/Latimeria_chalumnae.LatCha1.dna.toplevel.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/takifugu_rubripes/dna/Takifugu_rubripes.FUGU4.dna.toplevel.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/petromyzon_marinus/dna/Petromyzon_marinus.Pmarinus_7.0.dna.toplevel.fa.gz\r\nwget ftp://ftp.ensemblgenomes.org/pub/metazoa/release-26/fasta/strongylocentrotus_purpuratus/dna/Strongylocentrotus_purpuratus.GCA_000002235.2.26.dna.toplevel.fa.gz\r\n\r\ngunzip *.gz\r\n```\r\n\r\nWe'll also need the full collection of protein sequences from these organisms:\r\n```bash\r\nmkdir proteins\r\ncd proteins\r\n\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/rattus_norvegicus/pep/Rattus_norvegicus.Rnor_6.0.pep.all.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/homo_sapiens/pep/Homo_sapiens.GRCh38.pep.all.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/sus_scrofa/pep/Sus_scrofa.Sscrofa10.2.pep.all.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/loxodonta_africana/pep/Loxodonta_africana.loxAfr3.pep.all.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/ornithorhynchus_anatinus/pep/Ornithorhynchus_anatinus.OANA5.pep.all.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/gallus_gallus/pep/Gallus_gallus.Galgal4.pep.all.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/xenopus_tropicalis/pep/Xenopus_tropicalis.JGI_4.2.pep.all.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/anolis_carolinensis/pep/Anolis_carolinensis.AnoCar2.0.pep.all.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/latimeria_chalumnae/pep/Latimeria_chalumnae.LatCha1.pep.all.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/takifugu_rubripes/pep/Takifugu_rubripes.FUGU4.pep.all.fa.gz\r\nwget ftp://ftp.ensembl.org/pub/release-80/fasta/petromyzon_marinus/pep/Petromyzon_marinus.Pmarinus_7.0.pep.all.fa.gz\r\nwget ftp://ftp.ensemblgenomes.org/pub/metazoa/release-26/fasta/strongylocentrotus_purpuratus/pep/Strongylocentrotus_purpuratus.GCA_000002235.2.26.pep.all.fa.gz\r\n\r\ngunzip *\r\n```\r\n\r\nWe're going to use exonerate protein2genome to map all of these proteins to their respective genomes.\r\n```bash\r\ncd ../\r\n\r\nfasta2esd --softmask no Loxodonta_africana.loxAfr3.dna.toplevel.fa Loxodonta_africana.loxAfr3.dna.toplevel.esd\r\nfasta2esd --softmask no Ornithorhynchus_anatinus.OANA5.dna.toplevel.fa Ornithorhynchus_anatinus.OANA5.dna.toplevel.esd\r\nfasta2esd --softmask no Strongylocentrotus_purpuratus.GCA_000002235.2.26.dna.toplevel.fa Strongylocentrotus_purpuratus.GCA_000002235.2.26.dna.toplevel.esd\r\nfasta2esd --softmask no Petromyzon_marinus.Pmarinus_7.0.dna.toplevel.fa Petromyzon_marinus.Pmarinus_7.0.dna.toplevel.esd\r\nfasta2esd --softmask no Takifugu_rubripes.FUGU4.dna.toplevel.fa Takifugu_rubripes.FUGU4.dna.toplevel.esd\r\nfasta2esd --softmask no Latimeria_chalumnae.LatCha1.dna.toplevel.fa Latimeria_chalumnae.LatCha1.dna.toplevel.esd\r\nfasta2esd --softmask no Anolis_carolinensis.AnoCar2.0.dna.toplevel.fa Anolis_carolinensis.AnoCar2.0.dna.toplevel.esd\r\nfasta2esd --softmask no Xenopus_tropicalis.JGI_4.2.dna.toplevel.fa Xenopus_tropicalis.JGI_4.2.dna.toplevel.esd\r\nfasta2esd --softmask no Gallus_gallus.Galgal4.dna.toplevel.fa Gallus_gallus.Galgal4.dna.toplevel.esd\r\nfasta2esd --softmask no Sus_scrofa.Sscrofa10.2.dna.toplevel.fa Sus_scrofa.Sscrofa10.2.dna.toplevel.esd\r\nfasta2esd --softmask no Homo_sapiens.GRCh38.dna.primary_assembly.fa Homo_sapiens.GRCh38.dna.primary_assembly.esd\r\nfasta2esd --softmask no Rattus_norvegicus.Rnor_6.0.dna.toplevel.fa Rattus_norvegicus.Rnor_6.0.dna.toplevel.esd\r\n\r\nesd2esi Loxodonta_africana.loxAfr3.dna.toplevel.esd Loxodonta_africana.loxAfr3.dna.toplevel.trans.esi --translate yes --memorylimit 25600\r\nesd2esi Ornithorhynchus_anatinus.OANA5.dna.toplevel.esd Ornithorhynchus_anatinus.OANA5.dna.toplevel.trans.esi --translate yes --memorylimit 25600\r\nesd2esi Strongylocentrotus_purpuratus.GCA_000002235.2.26.dna.toplevel.esd Strongylocentrotus_purpuratus.GCA_000002235.2.26.dna.toplevel.trans.esi --translate yes --memorylimit 25600\r\nesd2esi Petromyzon_marinus.Pmarinus_7.0.dna.toplevel.esd Petromyzon_marinus.Pmarinus_7.0.dna.toplevel.trans.esi --translate yes --memorylimit 25600\r\nesd2esi Takifugu_rubripes.FUGU4.dna.toplevel.esd Takifugu_rubripes.FUGU4.dna.toplevel.trans.esi --translate yes --memorylimit 25600\r\nesd2esi Latimeria_chalumnae.LatCha1.dna.toplevel.esd Latimeria_chalumnae.LatCha1.dna.toplevel.trans.esi --translate yes --memorylimit 25600\r\nesd2esi Anolis_carolinensis.AnoCar2.0.dna.toplevel.esd Anolis_carolinensis.AnoCar2.0.dna.toplevel.trans.esi --translate yes --memorylimit 25600\r\nesd2esi Xenopus_tropicalis.JGI_4.2.dna.toplevel.esd Xenopus_tropicalis.JGI_4.2.dna.toplevel.trans.esi --translate yes --memorylimit 25600\r\nesd2esi Gallus_gallus.Galgal4.dna.toplevel.esd Gallus_gallus.Galgal4.dna.toplevel.trans.esi --translate yes --memorylimit 25600\r\nesd2esi Sus_scrofa.Sscrofa10.2.dna.toplevel.esd Sus_scrofa.Sscrofa10.2.dna.toplevel.trans.esi --translate yes --memorylimit 25600\r\nesd2esi Homo_sapiens.GRCh38.dna.primary_assembly.esd Homo_sapiens.GRCh38.dna.primary_assembly.trans.esi --translate yes --memorylimit 25600\r\nesd2esi Rattus_norvegicus.Rnor_6.0.dna.toplevel.esd Rattus_norvegicus.Rnor_6.0.dna.toplevel.trans.esi --translate yes --memorylimit 25600\r\n```\r\n\r\nThe above will take several hours. We want to map the orthologous proteins from each species to their own reference genomes. So, we will need to find the orthologous proteins from each species. First we'll make blast databases from each protein set:\r\n```bash\r\ncd proteins\r\nfor i in *.pep.all.fa; do makeblastdb -in $i -dbtype prot; done\r\n```\r\n\r\nNow we'll iterate through each of the protein sets and blastx the mouse cDNAs to each, only outputting the best match:\r\n\r\n```bash\r\nfor i in *.pep.all.fa; do OUTFILE=$i\".proteinMatches\"; blastx -db $i -query ../../mmGRCm38.cdna.rand10kLongest.fa -outfmt 6 -max_target_seqs 1 -num_threads 10 -out $OUTFILE ; done\r\n```\r\n\r\nThe blastx'ing will also take a few hours.\r\n\r\nAfter the blastx is done, we'll parse the blastx results to pull out all the protein sequences that matched and store them into a new file for exonerate protein2genome mapping:\r\n\r\n```perl\r\n#!/usr/bin/perl\r\n\r\n# pullOutMatchingProteins.pl\r\n\r\nuse strict;\r\nuse warnings;\r\nuse Bio::SeqIO;\r\nuse Bio::SearchIO;\r\n\r\nmy @speciesArray = (\"Strongylocentrotus_purpuratus.GCA_000002235.2.26\", \"Petromyzon_marinus.Pmarinus_7.0\", \"Takifugu_rubripes.FUGU4\", \"Latimeria_chalumnae.LatCha1\", \"Anolis_carolinensis.AnoCar2.0\", \"Xenopus_tropicalis.JGI_4.2\", \"Gallus_gallus.Galgal4\", \"Ornithorhynchus_anatinus.OANA5\", \"Loxodonta_africana.loxAfr3\", \"Sus_scrofa.Sscrofa10.2\", \"Homo_sapiens.GRCh38\", \"Rattus_norvegicus.Rnor_6.0\");\r\n\r\nforeach my $species (@speciesArray) {\r\n    my $proteinFasta = $species . \".pep.all.fa\";\r\n    \r\n    my %protHash;\r\n    my $seqIn = Bio::SeqIO->new(-file => $proteinFasta,\r\n                                -format => 'fasta');\r\n    while (my $seq = $seqIn->next_seq()) {\r\n        $protHash{$seq->display_id()} = $seq;\r\n    }\r\n    \r\n    my $proteinOut = $species . \".pep.matching.fa\";\r\n    my $seqOut = Bio::SeqIO->new(-file => \">$proteinOut\",\r\n                                 -format => 'fasta');\r\n    \r\n    my $blastResults = $species . \".pep.all.fa.proteinMatches\";\r\n    open(my $blastFH, \"<\", $blastResults) or die \"Couldn't open $blastResults for reading: $!\\n\";\r\n    while (my $line = <$blastFH>) {\r\n        my @fields = split(/\\t/, $line);\r\n        $seqOut->write_seq($protHash{$fields[1]});\r\n    }\r\n}\r\n```\r\n\r\nNow we'll set these aside:\r\n\r\n```bash\r\nmkdir matchingProteins\r\nmv *.pep.matching.fa matchingProteins/\r\n```\r\n\r\nExonerate protein2genome is very slow unless you change some settings around, and it seems to run faster when you separate each sequence into its own file. Using only one sequence per run also has the benefit of showing you where you left off if exonerate segfaults.\r\n\r\nSo we're first gonna need to separate the protein fasta files into new files, one sequence per file:\r\n\r\n```perl\r\n#!/usr/bin/perl\r\n\r\n# separateProteinsIntoIndividuals.pl\r\n\r\nuse strict;\r\nuse warnings;\r\nuse Bio::SeqIO;\r\nuse Bio::SearchIO;\r\n\r\nmy @speciesArray = (\"Strongylocentrotus_purpuratus.GCA_000002235.2.26\", \"Petromyzon_marinus.Pmarinus_7.0\", \"Takifugu_rubripes.FUGU4\", \"Latimeria_chalumnae.LatCha1\", \"Anolis_carolinensis.AnoCar2.0\", \"Xenopus_tropicalis.JGI_4.2\", \"Gallus_gallus.Galgal4\", \"Ornithorhynchus_anatinus.OANA5\", \"Loxodonta_africana.loxAfr3\", \"Sus_scrofa.Sscrofa10.2\", \"Homo_sapiens.GRCh38\", \"Rattus_norvegicus.Rnor_6.0\");\r\n\r\nforeach my $species (@speciesArray) {\r\n    # We'll have a different folder for each species\r\n    unless (-d $species) {\r\n        mkdir $species;\r\n    }\r\n    chdir $species;\r\n    \r\n    my $seqsFile = \"../\" . $species . \".pep.matching.fa\";\r\n\r\n    my $seqIn = Bio::SeqIO->new(-file => $seqsFile,\r\n                                -format => 'fasta');\r\n    \r\n    my $counter = 0;\r\n    while (my $seq = $seqIn->next_seq()) {\r\n        $counter++;\r\n        my $seqOutName = $seq->display_id() . \".pep.fasta\";\r\n        my $seqOut = Bio::SeqIO->new(-file => \">$seqOutName\",\r\n                                     -format => 'fasta');\r\n        \r\n        $seqOut->write_seq($seq);\r\n        \r\n    }\r\n    print $counter . \" total records processed\\n\";\r\n    chdir \"..\";\r\n}\r\n```\r\n\r\nNote that some mouse transcripts might have selected the same genes from the reference set as others for the best blastx hit. This means that there should be fewer actual peptide fastas than query sequences that had positive matches in the blastx search.\r\n\r\nNow let's loop through all those sequences and align them to their proper genomes. The sending multiple queries to a single exonerate-server tends to make things segfault, so instead we'll instantiate exonerate-servers so that each thread gets its own.\r\n\r\n\r\n```perl\r\n#!/usr/bin/perl\r\n\r\n# exonerateP2G.pl\r\n\r\nuse strict;\r\nuse warnings;\r\nuse Bio::SeqIO;\r\nuse Bio::SearchIO;\r\nuse Parallel::ForkManager;\r\n\r\nmy @speciesArray = (\"Strongylocentrotus_purpuratus.GCA_000002235.2.26\", \"Petromyzon_marinus.Pmarinus_7.0\", \"Takifugu_rubripes.FUGU4\", \"Latimeria_chalumnae.LatCha1\", \"Anolis_carolinensis.AnoCar2.0\", \"Xenopus_tropicalis.JGI_4.2\", \"Gallus_gallus.Galgal4\", \"Ornithorhynchus_anatinus.OANA5\", \"Loxodonta_africana.loxAfr3\", \"Sus_scrofa.Sscrofa10.2\", \"Homo_sapiens.GRCh38\", \"Rattus_norvegicus.Rnor_6.0\");\r\n\r\n\r\nmy $portCounter = 12886; # We'll start on port 12887 and go up 29 more\r\n\r\nforeach my $species (@speciesArray) {\r\n    my $genomeFile = $species . \".dna.toplevel.trans.esi\";\r\n    \r\n    # We'll use a total of 15 possible threads for the forkmanager, because\r\n    # each thread will initiate two processes--the exonerate server and the \r\n    # exonerate clients. We want to use about 30 CPUs total, so set this\r\n    # to 15.\r\n    my $forkManager = Parallel::ForkManager->new(15);\r\n    \r\n    foreach my $thread (0..14) {\r\n        $portCounter++;\r\n        $forkManager->start and next;\r\n        chdir(\"/home/evan/spliceFinder/genomes/\");\r\n        system(\"exonerate-server --port $portCounter $genomeFile &\");\r\n        sleep(30);\r\n        chdir(\"proteins/matchingProteins/\");\r\n        chdir $species;\r\n        unless(-d \"exonerate\") {\r\n            mkdir \"exonerate\";\r\n        }\r\n\r\n        opendir(my $dirFH, \"./\");\r\n        my @files = readdir($dirFH);\r\n        closedir($dirFH);\r\n\r\n        my @fastaFiles;\r\n        foreach my $file (@files) {\r\n            if ($file =~ /.fasta/) {\r\n                push(@fastaFiles, $file);\r\n            }\r\n        }\r\n\r\n        my $proteinsPerThread = scalar(@fastaFiles) / 15; # Decide how many proteins to provide to each thread\r\n\r\n        my $beginning = $thread * $proteinsPerThread;\r\n        my $end = ($thread+1) * $proteinsPerThread;\r\n        for (my $fileIndex=$beginning; $fileIndex < $end; $fileIndex++) {\r\n            my $outFileName = \"exonerate/\" . $fastaFiles[$fileIndex] . \".p2g.exonerate\";\r\n            system(\"exonerate $fastaFiles[$fileIndex] localhost:$portCounter --querytype protein --targettype dna --model p2g --bestn 1 --dnawordlen 12 --fsmmemory 4096 --hspfilter 50 --geneseed 250 > $outFileName\");\r\n        }\r\n        $forkManager->finish;\r\n    }\r\n    $forkManager->wait_all_children;\r\n    sleep(120);\r\n    system(\"pkill -f exonerate-server\");\r\n    sleep(120);\r\n}\r\n```\r\n\r\nThis takes several hours to a day to run all these, assuming all goes well and you have > 30 cores to work with.\r\n\r\nAfter this comes one of the trickier parts. We'll process all of these exonerate files to harvest the putative contiguous genomic region sequences and align to the proteins. For instance, rat protein ENSRNOP00000046335 was mapped to the rat chromosome 7 between base pairs 143497108 and 143489162. Additionally, exonerate uncovered 8 introns spread throughout the alignment. The first intron starts at about bp 143496581 and ends at bp 143495791. So, we'll designate the first intron as running from 143496581-143497108. We want to pull the rat genomic DNA sequence from 143496581-143497108 and align it to the original mouse EST sequence. \r\n\r\nLet's say this exon sequence aligns at base pairs 10-537 in the mouse EST. We would then code this as rat-inferred splice sites before base 10 and after 537 in the mouse EST. We'll gather all such inferred splice sites for each EST, keeping track of which ones come from which species.\r\n\r\nI'll present this all in heavily-commented code below.\r\n\r\n```perl\r\n#!/usr/bin/perl\r\n\r\n# harvestExonsAndAlign.pl\r\n\r\nuse strict;\r\nuse warnings;\r\nuse Bio::SearchIO; # <= We'll use this to parse the exonerate reports\r\nuse Bio::SeqIO;\r\n\r\n\r\n# We'll deal with all the different reference genomes one-by-one in a loop\r\nmy @speciesArray = (\"Strongylocentrotus_purpuratus.GCA_000002235.2.26\", \"Petromyzon_marinus.Pmarinus_7.0\", \"Takifugu_rubripes.FUGU4\", \"Latimeria_chalumnae.LatCha1\", \"Anolis_carolinensis.AnoCar2.0\", \"Xenopus_tropicalis.JGI_4.2\", \"Gallus_gallus.Galgal4\", \"Ornithorhynchus_anatinus.OANA5\", \"Loxodonta_africana.loxAfr3\", \"Sus_scrofa.Sscrofa10.2\", \"Homo_sapiens.GRCh38\", \"Rattus_norvegicus.Rnor_6.0\");\r\nforeach my $species (@speciesArray) {\r\n    chdir($species);\r\n        \r\n}\r\n```\r\n    \r\n    \r\n    \r\n    \r\n    \r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}